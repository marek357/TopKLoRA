model:
  name: llama
  version: 3.2
  size: 1B
  huggingface_model_id: meta-llama/Llama-3.2-1B-Instruct
training:
  method: sft
  sft:
    enabled: true
    batch_size_train: 12
    batch_size_eval: 12
    num_epochs: 10
    gradient_checkpointing: true
    push_to_hub: false
    max_seq_length: 2048
    weight_decay: 0.01
    max_steps: 500
    save_steps: 500
    lr: 5.0e-05
  dpo:
    enabled: false
dataset_sft:
  name: helpfulness
  huggingface_dataset_id: trl-lib/ultrafeedback-gpt-3.5-turbo-helpfulness
dataset_dpo:
  name: binarized
  huggingface_dataset_id: trl-lib/ultrafeedback_binarized
optimizer:
  type: adamw
  weight_decay: 0.01
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
experiment:
  size: medium
  lora:
    r: 16
    alpha: 18
    dropout: 0.05
    target_modules:
    - q_proj
    - k_proj
lr_scheduler:
  type: cosine
logger:
  wandb_mode: disabled
  project: llm-lora-dpo
  entity: my-entity
  log_model: false
  logging_steps: 10
  report_to: wandb
seed: 42
experiment_name: ${training.method}_${model.name}_${model.version}_${model.size}_${experiment.size}
