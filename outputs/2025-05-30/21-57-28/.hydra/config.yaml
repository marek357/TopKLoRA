model:
  name: opt
  version: 1.0
  size: 350M
  huggingface_model_id: facebook/opt-350m
training:
  method: sft
  sft:
    enabled: true
    batch_size_train: 8
    batch_size_eval: 8
    num_epochs: 1
    gradient_checkpointing: true
    push_to_hub: false
    max_seq_length: 2048
    weight_decay: 0.01
    max_steps: 50
    save_steps: 50
    lr: 0.0001
  dpo:
    enabled: false
dataset:
  name: helpfulness
  huggingface_dataset_id: trl-lib/ultrafeedback-gpt-3.5-turbo-helpfulness
optimizer:
  type: adamw
  weight_decay: 0.01
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
experiment:
  size: small
  lora:
    r: 6
    alpha: 12
    dropout: 0.05
    target_modules:
    - fc_out
lr_scheduler:
  type: cosine
logger:
  wandb_mode: disabled
  project: llm-lora-dpo
  entity: my-entity
  log_model: false
  logging_steps: 10
  report_to: wandb
seed: 42
experiment_name: ${training.method}_${model.name}_${model.version}_${model.size}_${experiment.size}
