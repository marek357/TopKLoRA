# Enhanced TopK LoRA experiment with aggressive sparsity
# High sparsity with temperature annealing

size: "enhanced_sparse"
lora:
  r: 1024
  bias: 'none'
  alpha: 1024
  dropout: 0.05
  
  # Enable TopK functionality with high sparsity
  use_topk: true
  k: 64              # Keep top 16 latents active (87.5% sparsity)
  k_final: 8         # Final k value (even sparser)
  k_schedule: "cubic"  # Cubic schedule for gradual sparsification
  
  # Aggressive temperature scheduling
  temperature: 1.0              # Higher initial temperature
  temperature_final: 0.01       # Very low final temperature 
  temperature_schedule: "exp"   # Exponential decay for sharp transitions
  
  # Enhanced monitoring
  log_dead_latents: false
  dead_latents_log_every: 250   # More frequent logging
  
  # Target single layer for focused analysis
  target_modules:
    # - layers.13.self_attn.q_proj
    # - layers.13.self_attn.k_proj
    # - layers.13.self_attn.v_proj
    # - layers.13.self_attn.o_proj
    - layers.13.mlp.gate_proj
    - layers.13.mlp.up_proj
    - layers.13.mlp.down_proj
