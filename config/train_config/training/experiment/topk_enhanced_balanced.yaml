# Enhanced TopK LoRA experiment with moderate sparsity
# Balanced configuration for good performance vs interpretability

size: "enhanced_balanced"
lora:
  r: 64
  bias: 'none'
  alpha: 64
  dropout: 0.05
  
  # Enable TopK functionality with moderate sparsity
  use_topk: true
  k: 24              # Keep top 24 latents active (62.5% sparsity)
  k_final: 16        # Final k value (75% sparsity)
  k_schedule: "linear"  # Linear transition
  
  # Moderate temperature scheduling
  temperature: 1.0              # Standard initial temperature
  temperature_final: 0.05       # Low final temperature 
  temperature_schedule: "cubic" # Cubic schedule for smooth transition
  
  # Standard monitoring
  log_dead_latents: true
  dead_latents_log_every: 500
  
  # Target key modules for balanced coverage
  target_modules:
    - layers.10.self_attn.q_proj
    - layers.10.self_attn.v_proj
    - layers.10.mlp.gate_proj
    - layers.10.mlp.up_proj
    - layers.11.self_attn.q_proj
    - layers.11.self_attn.v_proj
    - layers.11.mlp.gate_proj
    - layers.11.mlp.up_proj
    - layers.12.self_attn.q_proj
    - layers.12.self_attn.v_proj
    - layers.12.mlp.gate_proj
    - layers.12.mlp.up_proj
