name: auto-interpret

activation_collection:
  enabled: true
  n_tokens: 5000
  seq_len: 2048 # max sequence length (longer prompts are truncated by the collator)
  top_prompt_count: 1000
  max_batches: 500000
  batch_size: 1
  dataset_name: "lmsys/lmsys-chat-1m"
  dataset_split: train
  dataset_continuation: null # not applicable for LMSYS format (no chosen/rejected)
  dataset_config_name: null
  compute_co_occurrence: true # compute latent co-occurrence statistics across all hookpoints

# selection of top latents to analyse
latent_selection:
  enabled: true
  max_latents: 128
  p_active_min: 0.01
  p_active_max: 0.9

# delphi scoring
delphi_scoring:
  enabled: false
  max_concurrent: 1 # Maximum concurrent pipeline tasks (applies to all providers)

  use_openai_simulator: false

  scoring_client:
    provider: openai # offline | openai
    openai_config:
      # openai config
      openai_model: "gpt-4o-mini"
      openai_base_url: null
      openai_temperature: 0.0
      openai_max_tokens: 3000
      openai_timeout: 60
      cost_monitor_enabled: true
      cost_monitor_every_n_requests: 10
      # Set costs per 1M tokens for estimated spend
      input_cost_per_1m: 0.15
      output_cost_per_1m: 0.6

    offline_config:
      # model_name: Qwen/Qwen3-30B-A3B-Thinking-2507
      model_name: /Users/aszab/repos/models/OLMo-2-0425-1B-DPO
      max_model_len: 4096
      # max_model_len: 18000
      max_memory: 0.65
      prefix_caching: false
      batch_size: 1
      number_tokens_to_generate: 14500
      enforce_eager: true

seed: 42
build_eval_function:
  _target_: src.evals.auto_interp
