defaults:
  - quantization: 4bit
  - _self_
name: "gemma"
version: 2.0
size: "2B"
type: dpo_model_18layer
k: 64
r: 8192
base_model: "/Users/aszab/repos/TopKLoRA/models/sft-gemma2-2b-it"
model_it_name: "google/gemma-2-2b-it"
adapter_checkpoint_dir: "/Users/aszab/repos/TopKLoRA/adapter/r8192_k64_p5_mlp"
train_steps: 7500
